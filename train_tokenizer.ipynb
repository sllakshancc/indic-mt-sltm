{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d363c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbd2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-si\", cache_dir=\"./hf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316e46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 979109\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "{'en': 'Boone!', 'si': 'බුන්!'}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds[\"train\"][\"translation\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f48708",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_si = [ex[\"si\"] for ex in ds[\"train\"][\"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c82be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "# Sinhala tokenizer\n",
    "tokenizer_si = Tokenizer(models.BPE())\n",
    "tokenizer_si.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer_si = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"], vocab_size=50)\n",
    "tokenizer_si.train_from_iterator(train_texts_si[:10], trainer=trainer_si)\n",
    "\n",
    "# Save tokenizers\n",
    "tokenizer_si.save(\"tokenizer_si.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f57cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPE tokenizer on 100000 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting graphemes: 100%|██████████| 100000/100000 [00:03<00:00, 25549.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:   0%|          | 1/2837 [00:00<29:54,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: න් + න -> න්න\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:   4%|▎         | 101/2837 [00:51<22:18,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 100: බල + න්න -> බලන්න\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:   7%|▋         | 201/2837 [01:39<20:38,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 200: ම + ත් -> මත්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  11%|█         | 301/2837 [02:28<18:40,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 300: එයා + ගේ -> එයාගේ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  14%|█▍        | 401/2837 [03:11<17:47,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 400: දන්න + වද -> දන්නවද\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  18%|█▊        | 501/2837 [03:55<17:03,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 500: it + y -> ity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  21%|██        | 601/2837 [04:38<15:44,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 600: ර + හ -> රහ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  25%|██▍       | 701/2837 [05:20<15:11,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 700: උඹ + ට -> උඹට\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  28%|██▊       | 801/2837 [06:03<13:56,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 800: ර + ණය -> රණය\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  32%|███▏      | 901/2837 [06:46<15:10,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 900: හ + යි -> හයි\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  35%|███▌      | 1001/2837 [07:28<12:57,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1000: i + l -> il\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  39%|███▉      | 1101/2837 [08:10<12:11,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1100: තිබ් + බා -> තිබ්බා\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  42%|████▏     | 1201/2837 [08:51<11:14,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1200: සු + බ -> සුබ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  46%|████▌     | 1301/2837 [09:33<10:27,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1300: වෙ + ච්ච -> වෙච්ච\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  49%|████▉     | 1401/2837 [10:25<14:30,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1400: ක + න -> කන\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  53%|█████▎    | 1501/2837 [11:28<16:39,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1500: ජේ + න් -> ජේන්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  56%|█████▋    | 1601/2837 [12:29<11:59,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1600: ව + රෙන් -> වරෙන්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  60%|█████▉    | 1701/2837 [13:29<10:52,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1700: කො + ල්ලෝ -> කොල්ලෝ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  63%|██████▎   | 1801/2837 [14:35<10:12,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1800: ශක්ති + මත් -> ශක්තිමත්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  67%|██████▋   | 1901/2837 [15:40<10:30,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1900: අතා + රි -> අතාරි\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  71%|███████   | 2001/2837 [16:40<08:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2000: සෙ + න් -> සෙන්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  74%|███████▍  | 2101/2837 [17:49<07:34,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2100: උස + ස් -> උසස්\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  78%|███████▊  | 2201/2837 [18:55<06:11,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2200: පෙ + ළ -> පෙළ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  81%|████████  | 2301/2837 [19:54<05:16,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2300: මූ + ණ -> මූණ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  85%|████████▍ | 2401/2837 [20:52<04:10,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2400: ස්තු + ති -> ස්තුති\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  88%|████████▊ | 2501/2837 [21:59<03:19,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2500: සා + ම -> සාම\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  92%|█████████▏| 2601/2837 [23:02<02:14,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2600: ද + කුණ -> දකුණ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  95%|█████████▌| 2701/2837 [24:01<00:53,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2700: ප් + ට -> ප්ට\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:  99%|█████████▊| 2801/2837 [24:41<00:14,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 2800: වං + චා -> වංචා\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging: 100%|██████████| 2837/2837 [24:57<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Final vocab size: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'tokenizers')\n",
    "from GPETokenizer import GPETokenizer\n",
    "\n",
    "# 1. Create and train a tokenizer\n",
    "tokenizer = GPETokenizer(vocab_size=4000, dummy_prefix=None)\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(train_texts_si[:100000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897e9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2, 1167, 1, 1409, 1, 1395, 937, 32, 3]\n",
      "Decoded: මමගෙදරයනවා.\n",
      "Tokens: ['මම', '[UNK]', 'ගෙදර', '[UNK]', 'යන', 'වා', '.']\n",
      "Tokenizer saved to my_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# 2. Encode text to token IDs\n",
    "text = \"මම ගෙදර යනවා.\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Encoded: {token_ids}\")\n",
    "\n",
    "# 3. Decode token IDs back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "# 4. Tokenize text into subword strings\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# 6. Save and load the tokenizer\n",
    "tokenizer.save_pretrained_tokenizer_json(\"my_tokenizer.json\")  # Saves vocab and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dde0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tok_en = PreTrainedTokenizerFast(tokenizer_file=\"my_tokenizer.json\", bos_token=\"[BOS]\", eos_token=\"[EOS]\",\n",
    "                                 unk_token=\"[UNK]\", pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52494211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1167, 243, 1140, 1243, 2117, 1138, 32]\n",
      "Decoded: මම ග ෙ දර යනව ා .\n",
      "Tokens: ['මම', 'ග', 'ෙ', 'දර', 'යනව', 'ා', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"මම ගෙදර යනවා.\"\n",
    "token_ids = tok_en.encode(text)\n",
    "print(f\"Encoded: {token_ids}\")\n",
    "\n",
    "# 3. Decode token IDs back to text\n",
    "decoded_text = tok_en.decode(token_ids)\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "\n",
    "# 4. Tokenize text into subword strings\n",
    "tokens = tok_en.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
