{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDgl4KQRO-c4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains the base Tokenizer class and a few common helper functions.\n",
        "The base class also contains the (common) save/load functionality.\n",
        "It would be possible to be a lot more strict about the interface and\n",
        "e.g. isolating all regex/pattern parts to the RegexTokenizer, but\n",
        "some concessions are made for simplicity.\n",
        "\"\"\"\n",
        "import unicodedata\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# a few helper functions useful for both BasicTokenizer and RegexTokenizer\n",
        "\n",
        "def get_stats(ids, counts=None):\n",
        "    \"\"\"\n",
        "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
        "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
        "    Optionally allows to update an existing dictionary of counts\n",
        "    \"\"\"\n",
        "    counts = {} if counts is None else counts\n",
        "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    \"\"\"\n",
        "    In the list of integers (ids), replace all consecutive occurrences\n",
        "    of pair with the new integer token idx\n",
        "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
        "    \"\"\"\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        # if not at the very last position AND the pair matches, replace it\n",
        "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "# first two helper functions...\n",
        "def replace_control_characters(s: str) -> str:\n",
        "    # we don't want to print control characters\n",
        "    # which distort the output (e.g. \\n or much worse)\n",
        "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
        "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
        "    chars = []\n",
        "    for ch in s:\n",
        "        if unicodedata.category(ch)[0] != \"C\":\n",
        "            chars.append(ch) # this character is ok\n",
        "        else:\n",
        "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def render_token(t: bytes) -> str:\n",
        "    # pretty print a token, escaping control characters\n",
        "    s = t.decode('utf-8', errors='replace')\n",
        "    s = replace_control_characters(s)\n",
        "    return s\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# the base Tokenizer class\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Base class for Tokenizers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
        "        self.merges = {} # (int, int) -> int\n",
        "        self.pattern = \"\" # str\n",
        "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
        "        self.vocab = self._build_vocab() # int -> bytes\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Tokenizer can encode a string into a list of integers\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Tokenizer can decode a list of integers into a string\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        # vocab is simply and deterministically derived from merges\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        for special, idx in self.special_tokens.items():\n",
        "            vocab[idx] = special.encode(\"utf-8\")\n",
        "        return vocab\n",
        "\n",
        "    def save(self, file_prefix):\n",
        "        \"\"\"\n",
        "        Saves two files: file_prefix.vocab and file_prefix.model\n",
        "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
        "        - model file is the critical one, intended for load()\n",
        "        - vocab file is just a pretty printed version for human inspection only\n",
        "        \"\"\"\n",
        "        # write the model: to be used in load() later\n",
        "        model_file = file_prefix + \".model\"\n",
        "        with open(model_file, 'w') as f:\n",
        "            # write the version, pattern and merges, that's all that's needed\n",
        "            f.write(\"minbpe v1\\n\")\n",
        "            f.write(f\"{self.pattern}\\n\")\n",
        "            # write the special tokens, first the number of them, then each one\n",
        "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
        "            for special, idx in self.special_tokens.items():\n",
        "                f.write(f\"{special} {idx}\\n\")\n",
        "            # the merges dict\n",
        "            for idx1, idx2 in self.merges:\n",
        "                f.write(f\"{idx1} {idx2}\\n\")\n",
        "        # write the vocab: for the human to look at\n",
        "        vocab_file = file_prefix + \".vocab\"\n",
        "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for idx, token in self.vocab.items():\n",
        "                # note: many tokens may be partial utf-8 sequences\n",
        "                # and cannot be decoded into valid strings. Here we're using\n",
        "                # errors='replace' to replace them with the replacement char ï¿½.\n",
        "                # this also means that we couldn't possibly use .vocab in load()\n",
        "                # because decoding in this way is a lossy operation!\n",
        "                s = render_token(token)\n",
        "                # find the children of this token, if any\n",
        "                if idx in inverted_merges:\n",
        "                    # if this token has children, render it nicely as a merge\n",
        "                    idx0, idx1 = inverted_merges[idx]\n",
        "                    s0 = render_token(self.vocab[idx0])\n",
        "                    s1 = render_token(self.vocab[idx1])\n",
        "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
        "                else:\n",
        "                    # otherwise this is leaf token, just print it\n",
        "                    # (this should just be the first 256 tokens, the bytes)\n",
        "                    f.write(f\"[{s}] {idx}\\n\")\n",
        "\n",
        "    def load(self, model_file):\n",
        "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
        "        assert model_file.endswith(\".model\")\n",
        "        # read the model file\n",
        "        merges = {}\n",
        "        special_tokens = {}\n",
        "        idx = 256\n",
        "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
        "            # read the version\n",
        "            version = f.readline().strip()\n",
        "            assert version == \"minbpe v1\"\n",
        "            # read the pattern\n",
        "            self.pattern = f.readline().strip()\n",
        "            # read the special tokens\n",
        "            num_special = int(f.readline().strip())\n",
        "            for _ in range(num_special):\n",
        "                special, special_idx = f.readline().strip().split()\n",
        "                special_tokens[special] = int(special_idx)\n",
        "            # read the merges\n",
        "            for line in f:\n",
        "                idx1, idx2 = map(int, line.split())\n",
        "                merges[(idx1, idx2)] = idx\n",
        "                idx += 1\n",
        "        self.merges = merges\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab = self._build_vocab()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O5mrvbNPsGX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Minimal (byte-level) Byte Pair Encoding tokenizer.\n",
        "\n",
        "Algorithmically follows along the GPT tokenizer:\n",
        "https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "\n",
        "Unlike BasicTokenizer:\n",
        "- RegexTokenizer handles an optional regex splitting pattern.\n",
        "- RegexTokenizer handles optional special tokens.\n",
        "\"\"\"\n",
        "\n",
        "import regex as re\n",
        "\n",
        "\n",
        "# the main GPT text split patterns, see\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "\n",
        "\n",
        "class RegexTokenizer(Tokenizer):\n",
        "\n",
        "    def __init__(self, pattern=None):\n",
        "        \"\"\"\n",
        "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
        "        - special_tokens: str -> int dictionary of special tokens\n",
        "          example: {'<|endoftext|>': 100257}\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
        "        self.compiled_pattern = re.compile(self.pattern)\n",
        "        self.special_tokens = {}\n",
        "        self.inverse_special_tokens = {}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size >= 256\n",
        "        num_merges = vocab_size - 256\n",
        "\n",
        "        # split the text up into text chunks\n",
        "        text_chunks = re.findall(self.compiled_pattern, text)\n",
        "\n",
        "        # input text preprocessing\n",
        "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
        "\n",
        "        # iteratively merge the most common pairs to create new tokens\n",
        "        merges = {} # (int, int) -> int\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
        "        for i in range(num_merges):\n",
        "            # count the number of times every consecutive pair appears\n",
        "            stats = {}\n",
        "            for chunk_ids in ids:\n",
        "                # passing in stats will update it in place, adding up counts\n",
        "                get_stats(chunk_ids, stats)\n",
        "            # find the pair with the highest count\n",
        "            pair = max(stats, key=stats.get)\n",
        "            # mint a new token: assign it the next available id\n",
        "            idx = 256 + i\n",
        "            # replace all occurrences of pair in ids with idx\n",
        "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
        "            # save the merge\n",
        "            merges[pair] = idx\n",
        "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
        "            # prints\n",
        "            if verbose and num_merges % 100 == 0:\n",
        "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
        "\n",
        "        # save class variables\n",
        "        self.merges = merges # used in encode()\n",
        "        self.vocab = vocab   # used in decode()\n",
        "\n",
        "    def register_special_tokens(self, special_tokens):\n",
        "        # special_tokens is a dictionary of str -> int\n",
        "        # example: {\"<|endoftext|>\": 100257}\n",
        "        self.special_tokens = special_tokens\n",
        "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # given ids (list of integers), return Python string\n",
        "        part_bytes = []\n",
        "        for idx in ids:\n",
        "            if idx in self.vocab:\n",
        "                part_bytes.append(self.vocab[idx])\n",
        "            elif idx in self.inverse_special_tokens:\n",
        "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
        "            else:\n",
        "                raise ValueError(f\"invalid token id: {idx}\")\n",
        "        text_bytes = b\"\".join(part_bytes)\n",
        "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text\n",
        "\n",
        "    def _encode_chunk(self, text_bytes):\n",
        "        # return the token ids\n",
        "        # let's begin. first, convert all bytes to integers in range 0..255\n",
        "        ids = list(text_bytes)\n",
        "        while len(ids) >= 2:\n",
        "            # find the pair with the lowest merge index\n",
        "            stats = get_stats(ids)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "            # subtle: if there are no more merges available, the key will\n",
        "            # result in an inf for every single pair, and the min will be\n",
        "            # just the first pair in the list, arbitrarily\n",
        "            # we can detect this terminating case by a membership check\n",
        "            if pair not in self.merges:\n",
        "                break # nothing else can be merged anymore\n",
        "            # otherwise let's merge the best pair (lowest merge index)\n",
        "            idx = self.merges[pair]\n",
        "            ids = merge(ids, pair, idx)\n",
        "        return ids\n",
        "\n",
        "    def encode_ordinary(self, text):\n",
        "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
        "        # split text into chunks of text by categories defined in regex pattern\n",
        "        text_chunks = re.findall(self.compiled_pattern, text)\n",
        "        # all chunks of text are encoded separately, then results are joined\n",
        "        ids = []\n",
        "        for chunk in text_chunks:\n",
        "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
        "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
        "            ids.extend(chunk_ids)\n",
        "        return ids\n",
        "\n",
        "    def encode(self, text, allowed_special=\"none_raise\"):\n",
        "        \"\"\"\n",
        "        Unlike encode_ordinary, this function handles special tokens.\n",
        "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
        "        if none_raise, then an error is raised if any special token is encountered in text\n",
        "        this is the default tiktoken behavior right now as well\n",
        "        any other behavior is either annoying, or a major footgun\n",
        "        \"\"\"\n",
        "        # decode the user desire w.r.t. handling of special tokens\n",
        "        special = None\n",
        "        if allowed_special == \"all\":\n",
        "            special = self.special_tokens\n",
        "        elif allowed_special == \"none\":\n",
        "            special = {}\n",
        "        elif allowed_special == \"none_raise\":\n",
        "            special = {}\n",
        "            assert all(token not in text for token in self.special_tokens)\n",
        "        elif isinstance(allowed_special, set):\n",
        "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
        "        else:\n",
        "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
        "        if not special:\n",
        "            # shortcut: if no special tokens, just use the ordinary encoding\n",
        "            return self.encode_ordinary(text)\n",
        "        # otherwise, we have to be careful with potential special tokens in text\n",
        "        # we handle special tokens by splitting the text\n",
        "        # based on the occurrence of any exact match with any of the special tokens\n",
        "        # we can use re.split for this. note that surrounding the pattern with ()\n",
        "        # makes it into a capturing group, so the special tokens will be included\n",
        "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
        "        special_chunks = re.split(special_pattern, text)\n",
        "        # now all the special characters are separated from the rest of the text\n",
        "        # all chunks of text are encoded separately, then results are joined\n",
        "        ids = []\n",
        "        for part in special_chunks:\n",
        "            if part in special:\n",
        "                # this is a special token, encode it separately as a special case\n",
        "                ids.append(special[part])\n",
        "            else:\n",
        "                # this is an ordinary sequence, encode it normally\n",
        "                ids.extend(self.encode_ordinary(part))\n",
        "        return ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny-U84TPQxkO"
      },
      "outputs": [],
      "source": [
        "text = open(\"taylorswift.txt\", \"r\", encoding=\"utf-8\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMm6it0_QXrN",
        "outputId": "0e3c73d8-b45f-41d9-bc49-1938c02de1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[264, 302, 111, 341, 258, 108, 100]\n",
            "hello world\n"
          ]
        }
      ],
      "source": [
        "c_pattern = re.compile(r\" ?\\w+| ?[^\\w\\s]+\")\n",
        "tokenizer = RegexTokenizer(pattern=c_pattern)\n",
        "tokenizer.train(text, 512, verbose=True)\n",
        "token = tokenizer.encode(\"hello world\") # string -> tokens\n",
        "detoken = tokenizer.decode(token) # tokens -> string\n",
        "print(token)\n",
        "print(detoken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NvfvQrCVqcH",
        "outputId": "05eac4a0-fb8f-44e3-9856-917877b2a7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[264, 302, 111, 49, 50, 51, 33, 33, 33, 63, 295, 236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 33, 41, 32, 240, 159, 152, 137]\n",
            "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰\n",
            "[224, 183, 129, 224, 183, 138, 487, 141, 224, 182, 187, 224, 183, 147, 32, 224, 182, 189, 224, 182, 130, 224, 182, 154, 224, 183, 143, 224, 183, 128, 224, 183, 154, 32, 224, 183, 128, 224, 183, 146, 224, 183, 129, 224, 183, 143, 224, 182, 189, 224, 182, 173, 224, 183, 138, 224, 183, 128, 224, 182, 186, 32, 224, 183, 128, 224, 182, 187, 224, 183, 138, 224, 182, 156, 32, 224, 182, 154, 224, 183, 146, 46, 224, 182, 184, 224, 183, 147, 32, 54, 53, 44, 54, 49, 48, 295, 224, 183, 128, 224, 182, 187, 224, 183, 138, 224, 182, 156, 32, 224, 183, 131, 224, 183, 144, 224, 182, 173, 224, 182, 180, 224, 183, 148, 224, 182, 184, 224, 183, 138, 256, 53, 44, 51, 51, 48, 41, 32, 224, 183, 128, 224, 183, 154, 46]\n",
            "à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà·à·€à·š à·€à·’à·à·à¶½à¶­à·Šà·€à¶º à·€à¶»à·Šà¶œ à¶šà·’.à¶¸à·“ 65,610 (à·€à¶»à·Šà¶œ à·ƒà·à¶­à¶´à·”à¶¸à·Š 25,330) à·€à·š.\n"
          ]
        }
      ],
      "source": [
        "token = tokenizer.encode(\"hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰\") # string -> tokens\n",
        "detoken = tokenizer.decode(token) # tokens -> string\n",
        "print(token)\n",
        "print(detoken)\n",
        "\n",
        "token = tokenizer.encode(\"à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà·à·€à·š à·€à·’à·à·à¶½à¶­à·Šà·€à¶º à·€à¶»à·Šà¶œ à¶šà·’.à¶¸à·“ 65,610 (à·€à¶»à·Šà¶œ à·ƒà·à¶­à¶´à·”à¶¸à·Š 25,330) à·€à·š.\") # string -> tokens\n",
        "detoken = tokenizer.decode(token) # tokens -> string\n",
        "print(token)\n",
        "print(detoken)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
