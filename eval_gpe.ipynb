{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4c164d",
   "metadata": {},
   "source": [
    "### 1. Implement Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grapheme\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "class GraphemeBPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.grapheme_to_id = {}\n",
    "        self.id_to_grapheme = {}\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def _get_graphemes(self, text):\n",
    "        \"\"\"Extract graphemes from text using grapheme library\"\"\"\n",
    "        return list(grapheme.graphemes(text))\n",
    "    \n",
    "    def _get_word_tokens(self, text):\n",
    "        \"\"\"Split text into words and convert each word to grapheme sequence\"\"\"\n",
    "        # Simple word splitting - you might want to improve this for Sinhala\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        word_tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isspace():\n",
    "                word_tokens.append([word])\n",
    "            else:\n",
    "                graphemes = self._get_graphemes(word)\n",
    "                # Add end-of-word marker to distinguish word boundaries\n",
    "                graphemes.append('</w>')\n",
    "                word_tokens.append(graphemes)\n",
    "        \n",
    "        return word_tokens\n",
    "    \n",
    "    def _get_pairs(self, word_tokens):\n",
    "        \"\"\"Get all adjacent pairs of graphemes/tokens\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pairs[pair] += 1\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, pair, word_tokens):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_word_tokens = []\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                    # Merge the pair\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            new_word_tokens.append(new_word)\n",
    "        \n",
    "        return new_word_tokens\n",
    "    \n",
    "    def train(self, texts, vocab_size=1000):\n",
    "        \"\"\"Train BPE tokenizer starting with graphemes\"\"\"\n",
    "        print(\"Starting grapheme-based BPE training...\")\n",
    "        \n",
    "        # Step 1: Extract all words and convert to grapheme sequences\n",
    "        all_word_tokens = []\n",
    "        for text in texts:\n",
    "            word_tokens = self._get_word_tokens(text)\n",
    "            all_word_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Step 2: Initialize vocabulary with all graphemes\n",
    "        grapheme_freq = Counter()\n",
    "        for word_tokens in all_word_tokens:\n",
    "            for word in word_tokens:\n",
    "                for token in word:\n",
    "                    grapheme_freq[token] += 1\n",
    "        \n",
    "        # Create initial vocabulary\n",
    "        self.vocab = dict(grapheme_freq)\n",
    "        \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Sample graphemes: {list(self.vocab.keys())[:20]}\")\n",
    "        \n",
    "        # Step 3: BPE merging process\n",
    "        for i in range(vocab_size - len(self.vocab)):\n",
    "            pairs = self._get_pairs(all_word_tokens)\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge the pair\n",
    "            all_word_tokens = self._merge_vocab(best_pair, all_word_tokens)\n",
    "            \n",
    "            # Update vocabulary\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[merged_token] = pairs[best_pair]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Merge {i}: {best_pair} -> {merged_token} (freq: {pairs[best_pair]})\")\n",
    "        \n",
    "        # Create token mappings\n",
    "        self.grapheme_to_id = {token: i for i, token in enumerate(self.vocab.keys())}\n",
    "        self.id_to_grapheme = {i: token for token, i in self.grapheme_to_id.items()}\n",
    "        \n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text using trained BPE model\"\"\"\n",
    "        word_tokens = self._get_word_tokens(text)\n",
    "        \n",
    "        # Apply merges\n",
    "        for pair in self.merges:\n",
    "            word_tokens = self._merge_vocab(pair, word_tokens)\n",
    "        \n",
    "        # Convert to IDs\n",
    "        token_ids = []\n",
    "        for word in word_tokens:\n",
    "            for token in word:\n",
    "                if token in self.grapheme_to_id:\n",
    "                    token_ids.append(self.grapheme_to_id[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens - you might want to use UNK token\n",
    "                    pass\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.id_to_grapheme:\n",
    "                tokens.append(self.id_to_grapheme[token_id])\n",
    "        \n",
    "        # Join tokens and remove end-of-word markers\n",
    "        text = ''.join(tokens).replace('</w>', ' ')\n",
    "        return text.strip()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'grapheme_to_id': self.grapheme_to_id,\n",
    "            'id_to_grapheme': self.id_to_grapheme\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.merges = [tuple(merge) for merge in model_data['merges']]\n",
    "        self.grapheme_to_id = model_data['grapheme_to_id']\n",
    "        self.id_to_grapheme = {int(k): v for k, v in model_data['id_to_grapheme'].items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeda764",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"ai4bharat/samanantar\", \"ta\", split=\"train\", streaming=True)\n",
    "\n",
    "subset = list(islice(ds, 10000))\n",
    "subset_ds = Dataset.from_list(subset)\n",
    "\n",
    "print(subset_ds)\n",
    "print(subset_ds[0])\n",
    "\n",
    "# Extract Tamil texts (target side)\n",
    "tamil_texts = [item['tgt'] for item in subset_ds]\n",
    "\n",
    "# Split into train (8K) and eval (2K) sets\n",
    "train_texts_ta = tamil_texts[:8000]\n",
    "eval_texts_ta = tamil_texts[8000:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddeca3",
   "metadata": {},
   "source": [
    "### 3. Implement calculating compression ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression_ratio(tokenizer, texts):\n",
    "    \"\"\"Calculate compression ratio for a list of texts\"\"\"\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Count characters (including spaces)\n",
    "        char_count = len(text)\n",
    "        \n",
    "        # Count tokens\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        token_count = len(token_ids)\n",
    "        \n",
    "        total_chars += char_count\n",
    "        total_tokens += token_count\n",
    "    \n",
    "    # Compression ratio = original_size / compressed_size\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_characters': total_chars,\n",
    "        'total_tokens': total_tokens,\n",
    "        'compression_ratio': compression_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd76f4",
   "metadata": {},
   "source": [
    "### 4. function for train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988289e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_grapheme_bpe(train_texts, lang, vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Train GraphemeBPE tokenizer from your dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your dataset with structure {'idx': int, 'src': str, 'tgt': str}\n",
    "        vocab_size: Desired vocabulary size\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training on {len(train_texts)} {lang} texts\")\n",
    "    \n",
    "    # Show sample texts and their graphemes\n",
    "    print(f\"\\nSample training text: {train_texts[0]}\")\n",
    "    sample_graphemes = list(grapheme.graphemes(train_texts[0]))\n",
    "    print(f\"Graphemes: {sample_graphemes}\")\n",
    "    print(f\"Number of graphemes: {len(sample_graphemes)}\")\n",
    "    \n",
    "    # Initialize and train tokenizer\n",
    "    tokenizer = GraphemeBPETokenizer()\n",
    "    print(f\"\\nTraining tokenizer with vocab_size={vocab_size}...\")\n",
    "    tokenizer.train(train_texts, vocab_size=vocab_size)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"{lang}_grapheme_bpe.json\"\n",
    "    tokenizer.save(model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    # Show vocabulary statistics\n",
    "    print(f\"\\nVocabulary Statistics:\")\n",
    "    print(f\"Total vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    print(f\"Number of merges performed: {len(tokenizer.merges)}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88775ebd",
   "metadata": {},
   "source": [
    "### 5. function for train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28839aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hf_bpe_tokenizer(train_texts, lang, vocab_size=1000):\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "    tokenizer.save(f\"{lang}_hf_bpe.json\")\n",
    "    print(\"\\nHuggingFace BPE tokenizer saved.\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb845ea1",
   "metadata": {},
   "source": [
    "### 6. Train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_ta = train_grapheme_bpe(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_ta = calculate_compression_ratio(gpe_tokenizer_ta, eval_texts_ta)\n",
    "print(\"\\nðŸ”¹ GPE Compression Ratio:\", gpe_stats_ta['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3d93e",
   "metadata": {},
   "source": [
    "### 7. Train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ae228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_ta = train_hf_bpe_tokenizer(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_ta = calculate_compression_ratio(hf_tokenizer_ta, eval_texts_ta)\n",
    "print(\"ðŸ”¹ HF BPE Compression Ratio:\", hf_stats_ta['compression_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250a03d",
   "metadata": {},
   "source": [
    "### 8. Overview of comparission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\nðŸ“Š Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_ta['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_ta['compression_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala Flores+ dataset\n",
    "sin_ds = load_dataset(\"openlanguagedata/flores_plus\", \"sin_Sinh\")\n",
    "train_texts_si = [item['text'] for item in sin_ds['dev']]\n",
    "eval_texts_si = [item['text'] for item in sin_ds['devtest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06becad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_si = train_grapheme_bpe(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_si = calculate_compression_ratio(gpe_tokenizer_si, eval_texts_si)\n",
    "print(\"\\nðŸ”¹ GPE Compression Ratio:\", gpe_stats_si['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a565b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_si = train_hf_bpe_tokenizer(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_si = calculate_compression_ratio(hf_tokenizer_si, eval_texts_si)\n",
    "print(\"ðŸ”¹ HF BPE Compression Ratio:\", hf_stats_si['compression_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\nðŸ“Š Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_si['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_si['compression_ratio']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
