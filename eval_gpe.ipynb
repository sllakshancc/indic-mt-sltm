{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4c164d",
   "metadata": {},
   "source": [
    "### 1. Implement Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77aa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aeda764",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\4th year\\1st sem\\Research Project\\experiments\\gpe-reproduce\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'idx': 0, 'src': 'Some 14 months later, the second calf is born.', 'tgt': '‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç 14 ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ, ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æï‡Æ©‡Øç‡Æ±‡Øà ‡Æà‡Æ©‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84ddeca3",
   "metadata": {},
   "source": [
    "### 3. Implement calculating compression ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d1b9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cbd76f4",
   "metadata": {},
   "source": [
    "### 4. function for train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988289e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88775ebd",
   "metadata": {},
   "source": [
    "### 5. function for train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28839aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb845ea1",
   "metadata": {},
   "source": [
    "### 6. Train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 8000 tamil texts\n",
      "\n",
      "Sample training text: ‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç 14 ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ, ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æï‡Æ©‡Øç‡Æ±‡Øà ‡Æà‡Æ©‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
      "Graphemes: ['‡Æö‡ØÅ', '‡ÆÆ‡Ææ', '‡Æ∞‡Øç', ' ', '1', '4', ' ', '‡ÆÆ‡Ææ', '‡Æ§', '‡Æô‡Øç', '‡Æï', '‡Æ≥‡Øç', ' ', '‡Æï', '‡Æ¥‡Æø', '‡Æ§‡Øç', '‡Æ§‡ØÅ', ',', ' ', '‡Æá', '‡Æ∞', '‡Æ£‡Øç', '‡Æü‡Ææ', '‡ÆÆ‡Øç', ' ', '‡Æï', '‡Æ©‡Øç', '‡Æ±‡Øà', ' ', '‡Æà', '‡Æ©‡ØÅ', '‡Æï‡Æø', '‡Æ±', '‡Æ§‡ØÅ', '.']\n",
      "Number of graphemes: 35\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 188\n",
      "Sample graphemes: ['‡Æö', '‡ØÅ', '‡ÆÆ', '‡Ææ', '‡Æ∞', '‡Øç', '<', '/', 'w', '>', ' ', '1', '4', '‡Æ§', '‡Æô', '‡Æï', '‡Æ≥', '‡Æ¥', '‡Æø', ',']\n",
      "Merge 0: ('‡ÆÆ‡Øç', '</w>') -> ‡ÆÆ‡Øç</w> (freq: 6861)\n",
      "Merge 100: ('0', '</w>') -> 0</w> (freq: 368)\n",
      "Merge 200: ('‡Æö‡ØÜ‡ÆØ‡Øç', '‡ÆØ') -> ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ (freq: 209)\n",
      "Merge 300: ('‡Æµ‡Æø', '‡Æ∞‡ØÅ') -> ‡Æµ‡Æø‡Æ∞‡ØÅ (freq: 140)\n",
      "Merge 400: ('‡ÆØ‡ØÜ', '‡Æï‡Øã‡Æµ‡Ææ') -> ‡ÆØ‡ØÜ‡Æï‡Øã‡Æµ‡Ææ (freq: 111)\n",
      "Merge 500: ('‡Æï‡Øç', '‡Æï‡ØÇ') -> ‡Æï‡Øç‡Æï‡ØÇ (freq: 89)\n",
      "Merge 600: ('‡Æ™‡Øç‡Æ™‡Øà', '</w>') -> ‡Æ™‡Øç‡Æ™‡Øà</w> (freq: 75)\n",
      "Merge 700: ('‡Æ≥‡Ææ', '‡Æ≤‡Øç</w>') -> ‡Æ≥‡Ææ‡Æ≤‡Øç</w> (freq: 62)\n",
      "Merge 800: ('‡Æµ', '‡Æô‡Øç‡Æï‡Æø') -> ‡Æµ‡Æô‡Øç‡Æï‡Æø (freq: 55)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: tamil_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 812\n",
      "\n",
      "üîπ GPE Compression Ratio: 2.924777121008174\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd3d93e",
   "metadata": {},
   "source": [
    "### 7. Train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ae228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "üîπ HF BPE Compression Ratio: 2.9452891717743204\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c250a03d",
   "metadata": {},
   "source": [
    "### 8. Overview of comparission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.92\n",
      "HuggingFace BPE:    2.95\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2f2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06becad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 997 sinhala texts\n",
      "\n",
      "Sample training text: ‡∑É‡∂≥‡∑î‡∂Ø‡∑è ‡∂Ø‡∑í‡∂±, ‡∑É‡∑ä‡∂ß‡∑ê‡∂±‡∑ä‡∑Ü‡∂ª‡∑ä‡∂©‡∑ä ‡∑É‡∂ª‡∑É‡∑Ä‡∑í ‡∑Ä‡∑õ‡∂Ø‡∑ä‚Äç‡∂∫ ‡∂¥‡∑è‡∑É‡∂Ω‡∑ö ‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑è‡∂•‡∂∫‡∑ù, ‡∑É‡∑õ‡∂Ω‡∂∫‡∂ö ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂ë‡∂∫ ‡∑Ä‡∂ª‡∑ä‡∂ú ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂±‡∑Ä ‡∂Ø‡∑ù‡∑Ç ‡∂±‡∑í‡∂ª‡∑ä‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ö ‡∂∏‡∑ô‡∑Ä‡∂Ω‡∂∏‡∂ö‡∑ä: ‡∂ë‡∂±‡∂∏‡∑ä, ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂á‡∂∏‡∂ª‡∑í‡∂ö‡∑è‡∂±‡∑î ‡∂©‡∑ú‡∂Ω‡∂ª‡∑ä ‡∑Å‡∂≠‡∂∫‡∂ö ‡∂¥‡∂∏‡∂´ ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑í‡∂±‡∑ä ‡∂±‡∑í‡∑Ç‡∑ä‡∂¥‡∑è‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∑É‡∑è‡∂∏‡∑è‡∂±‡∑ä‚Äç‡∂∫ ‡∂â‡∂±‡∑ä‡∂ö‡∑ä‡∂¢‡∑ô‡∂ß‡∑ä ‡∂∏‡∑î‡∂Ø‡∑ä‚Äç‡∂ª‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫‡∂ö‡∑í‡∂±‡∑ä ‡∂∏‡∑î‡∂Ø‡∑ä‚Äç‡∂ª‡∂´‡∂∫ ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂â‡∂≠‡∑è ‡∂ö‡∑î‡∂©‡∑è ‡∂†‡∑í‡∂¥‡∂∫‡∂ö‡∑ä ‡∑É‡∑ú‡∂∫‡∑è‡∂ú‡∂≠‡∑ä ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∑Ä‡∑ö‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö‡∑Ñ.\n",
      "Graphemes: ['‡∑É', '‡∂≥‡∑î', '‡∂Ø‡∑è', ' ', '‡∂Ø‡∑í', '‡∂±', ',', ' ', '‡∑É‡∑ä', '‡∂ß‡∑ê', '‡∂±‡∑ä', '‡∑Ü', '‡∂ª‡∑ä', '‡∂©‡∑ä', ' ', '‡∑É', '‡∂ª', '‡∑É', '‡∑Ä‡∑í', ' ', '‡∑Ä‡∑õ', '‡∂Ø‡∑ä\\u200d', '‡∂∫', ' ', '‡∂¥‡∑è', '‡∑É', '‡∂Ω‡∑ö', ' ', '‡∑Ä‡∑í', '‡∂Ø‡∑ä\\u200d', '‡∂∫‡∑è', '‡∂•', '‡∂∫‡∑ù', ',', ' ', '‡∑É‡∑õ', '‡∂Ω', '‡∂∫', '‡∂ö', ' ', '‡∂Ü', '‡∂ö‡∑è', '‡∂ª', '‡∂∫', ' ', '‡∂Ö', '‡∂±‡∑î', '‡∑Ä', ' ', '‡∂ë', '‡∂∫', ' ', '‡∑Ä', '‡∂ª‡∑ä', '‡∂ú', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∂±', '‡∑Ä', ' ', '‡∂Ø‡∑ù', '‡∑Ç', ' ', '‡∂±‡∑í', '‡∂ª‡∑ä', '‡∂´', '‡∂∫', ' ', '‡∂ö‡∑í', '‡∂ª‡∑ì', '‡∂∏‡∑ö', ' ', '‡∂∏‡∑ô', '‡∑Ä', '‡∂Ω', '‡∂∏', '‡∂ö‡∑ä', ':', ' ', '‡∂ë', '‡∂±', '‡∂∏‡∑ä', ',', ' ', '‡∂ë', '‡∂ö', '‡∂ö‡∑ä', ' ', '‡∂á', '‡∂∏', '‡∂ª‡∑í', '‡∂ö‡∑è', '‡∂±‡∑î', ' ', '‡∂©‡∑ú', '‡∂Ω', '‡∂ª‡∑ä', ' ', '‡∑Å', '‡∂≠', '‡∂∫', '‡∂ö', ' ', '‡∂¥', '‡∂∏', '‡∂´', ' ', '‡∂∏‡∑î', '‡∂Ø', '‡∂Ω', '‡∂ö‡∑í', '‡∂±‡∑ä', ' ', '‡∂±‡∑í', '‡∑Ç‡∑ä', '‡∂¥‡∑è', '‡∂Ø', '‡∂±', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∑É‡∑è', '‡∂∏‡∑è', '‡∂±‡∑ä\\u200d', '‡∂∫', ' ', '‡∂â', '‡∂±‡∑ä', '‡∂ö‡∑ä', '‡∂¢‡∑ô', '‡∂ß‡∑ä', ' ', '‡∂∏‡∑î', '‡∂Ø‡∑ä\\u200d', '‡∂ª', '‡∂´', ' ', '‡∂∫', '‡∂±‡∑ä', '‡∂≠‡∑ä\\u200d', '‡∂ª', '‡∂∫', '‡∂ö‡∑í', '‡∂±‡∑ä', ' ', '‡∂∏‡∑î', '‡∂Ø‡∑ä\\u200d', '‡∂ª', '‡∂´', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∂â', '‡∂≠‡∑è', ' ', '‡∂ö‡∑î', '‡∂©‡∑è', ' ', '‡∂†‡∑í', '‡∂¥', '‡∂∫', '‡∂ö‡∑ä', ' ', '‡∑É‡∑ú', '‡∂∫‡∑è', '‡∂ú', '‡∂≠‡∑ä', ' ', '‡∂∂', '‡∑Ä', '‡∂ß', ' ', '‡∂±‡∑í', '‡∑Ä‡∑ö', '‡∂Ø', '‡∂±', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', '‡∑Ñ', '.']\n",
      "Number of graphemes: 197\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 158\n",
      "Sample graphemes: ['‡∑É', '‡∂≥', '‡∑î', '‡∂Ø', '‡∑è', '<', '/', 'w', '>', ' ', '‡∑í', '‡∂±', ',', '‡∑ä', '‡∂ß', '‡∑ê', '‡∑Ü', '‡∂ª', '‡∂©', '‡∑Ä']\n",
      "Merge 0: ('‡∂∫', '</w>') -> ‡∂∫</w> (freq: 1469)\n",
      "Merge 100: ('‡∂¥', '‡∑Ä') -> ‡∂¥‡∑Ä (freq: 82)\n",
      "Merge 200: ('‡∂ú', '‡∂≠‡∑ä</w>') -> ‡∂ú‡∂≠‡∑ä</w> (freq: 47)\n",
      "Merge 300: ('‡∂ë', '‡∂∏</w>') -> ‡∂ë‡∂∏</w> (freq: 34)\n",
      "Merge 400: ('‡∂Ö', '‡∂©‡∑î</w>') -> ‡∂Ö‡∂©‡∑î</w> (freq: 24)\n",
      "Merge 500: ('‡∑Å', '‡∂ö‡∑ä') -> ‡∑Å‡∂ö‡∑ä (freq: 20)\n",
      "Merge 600: ('‡∂ö‡∑è', '‡∂ª‡∑ä') -> ‡∂ö‡∑è‡∂ª‡∑ä (freq: 17)\n",
      "Merge 700: ('‡∂ö‡∂ª', '‡∂±‡∑ä‡∂±‡∑ö</w>') -> ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö</w> (freq: 14)\n",
      "Merge 800: ('‡∂Ö', '‡∂ª‡∑ä') -> ‡∂Ö‡∂ª‡∑ä (freq: 12)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: sinhala_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 842\n",
      "\n",
      "üîπ GPE Compression Ratio: 2.4359331735147087\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a565b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "üîπ HF BPE Compression Ratio: 2.6817242578612492\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6199bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.44\n",
      "HuggingFace BPE:    2.68\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
