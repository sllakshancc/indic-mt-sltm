{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4c164d",
   "metadata": {},
   "source": [
    "### 1. Implement Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea77aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grapheme\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "class GraphemeBPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.grapheme_to_id = {}\n",
    "        self.id_to_grapheme = {}\n",
    "        self.merges = []\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def _get_graphemes(self, text):\n",
    "        \"\"\"Extract graphemes from text using grapheme library\"\"\"\n",
    "        return list(grapheme.graphemes(text))\n",
    "    \n",
    "    def _get_word_tokens(self, text):\n",
    "        \"\"\"Split text into words and convert each word to grapheme sequence\"\"\"\n",
    "        # Simple word splitting - you might want to improve this for Sinhala\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        word_tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isspace():\n",
    "                word_tokens.append([word])\n",
    "            else:\n",
    "                graphemes = self._get_graphemes(word)\n",
    "                # Add end-of-word marker to distinguish word boundaries\n",
    "                graphemes.append('</w>')\n",
    "                word_tokens.append(graphemes)\n",
    "        \n",
    "        return word_tokens\n",
    "    \n",
    "    def _get_pairs(self, word_tokens):\n",
    "        \"\"\"Get all adjacent pairs of graphemes/tokens\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pairs[pair] += 1\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, pair, word_tokens):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_word_tokens = []\n",
    "        \n",
    "        for word in word_tokens:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n",
    "                    # Merge the pair\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            new_word_tokens.append(new_word)\n",
    "        \n",
    "        return new_word_tokens\n",
    "    \n",
    "    def train(self, texts, vocab_size=1000):\n",
    "        \"\"\"Train BPE tokenizer starting with graphemes\"\"\"\n",
    "        print(\"Starting grapheme-based BPE training...\")\n",
    "        \n",
    "        # Step 1: Extract all words and convert to grapheme sequences\n",
    "        all_word_tokens = []\n",
    "        for text in texts:\n",
    "            word_tokens = self._get_word_tokens(text)\n",
    "            all_word_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Step 2: Initialize vocabulary with all graphemes\n",
    "        grapheme_freq = Counter()\n",
    "        for word_tokens in all_word_tokens:\n",
    "            for word in word_tokens:\n",
    "                for token in word:\n",
    "                    grapheme_freq[token] += 1\n",
    "        \n",
    "        # Create initial vocabulary\n",
    "        self.vocab = dict(grapheme_freq)\n",
    "        \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Sample graphemes: {list(self.vocab.keys())[:20]}\")\n",
    "        \n",
    "        # Step 3: BPE merging process\n",
    "        for i in range(vocab_size - len(self.vocab)):\n",
    "            pairs = self._get_pairs(all_word_tokens)\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge the pair\n",
    "            all_word_tokens = self._merge_vocab(best_pair, all_word_tokens)\n",
    "            \n",
    "            # Update vocabulary\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[merged_token] = pairs[best_pair]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Merge {i}: {best_pair} -> {merged_token} (freq: {pairs[best_pair]})\")\n",
    "        \n",
    "        # Create token mappings\n",
    "        self.grapheme_to_id = {token: i for i, token in enumerate(self.vocab.keys())}\n",
    "        self.id_to_grapheme = {i: token for token, i in self.grapheme_to_id.items()}\n",
    "        \n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text using trained BPE model\"\"\"\n",
    "        word_tokens = self._get_word_tokens(text)\n",
    "        \n",
    "        # Apply merges\n",
    "        for pair in self.merges:\n",
    "            word_tokens = self._merge_vocab(pair, word_tokens)\n",
    "        \n",
    "        # Convert to IDs\n",
    "        token_ids = []\n",
    "        for word in word_tokens:\n",
    "            for token in word:\n",
    "                if token in self.grapheme_to_id:\n",
    "                    token_ids.append(self.grapheme_to_id[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens - you might want to use UNK token\n",
    "                    pass\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.id_to_grapheme:\n",
    "                tokens.append(self.id_to_grapheme[token_id])\n",
    "        \n",
    "        # Join tokens and remove end-of-word markers\n",
    "        text = ''.join(tokens).replace('</w>', ' ')\n",
    "        return text.strip()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        model_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'grapheme_to_id': self.grapheme_to_id,\n",
    "            'id_to_grapheme': self.id_to_grapheme\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.vocab = model_data['vocab']\n",
    "        self.merges = [tuple(merge) for merge in model_data['merges']]\n",
    "        self.grapheme_to_id = model_data['grapheme_to_id']\n",
    "        self.id_to_grapheme = {int(k): v for k, v in model_data['id_to_grapheme'].items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeda764",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e1da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\4th year\\1st sem\\Research Project\\experiments\\gpe-reproduce\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d973f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'src', 'tgt'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'idx': 0, 'src': 'Some 14 months later, the second calf is born.', 'tgt': '‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç 14 ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ, ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æï‡Æ©‡Øç‡Æ±‡Øà ‡Æà‡Æ©‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(\"ai4bharat/samanantar\", \"ta\", split=\"train\", streaming=True)\n",
    "\n",
    "subset = list(islice(ds, 10000))\n",
    "subset_ds = Dataset.from_list(subset)\n",
    "\n",
    "print(subset_ds)\n",
    "print(subset_ds[0])\n",
    "\n",
    "# Extract Tamil texts (target side)\n",
    "tamil_texts = [item['tgt'] for item in subset_ds]\n",
    "\n",
    "# Split into train (8K) and eval (2K) sets\n",
    "train_texts_ta = tamil_texts[:8000]\n",
    "eval_texts_ta = tamil_texts[8000:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddeca3",
   "metadata": {},
   "source": [
    "### 3. Implement calculating compression ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19d1b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression_ratio(tokenizer, texts):\n",
    "    \"\"\"Calculate compression ratio for a list of texts\"\"\"\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        # Count characters (including spaces)\n",
    "        char_count = len(text)\n",
    "        \n",
    "        # Count tokens\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        token_count = len(token_ids)\n",
    "        \n",
    "        total_chars += char_count\n",
    "        total_tokens += token_count\n",
    "    \n",
    "    # Compression ratio = original_size / compressed_size\n",
    "    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_characters': total_chars,\n",
    "        'total_tokens': total_tokens,\n",
    "        'compression_ratio': compression_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd76f4",
   "metadata": {},
   "source": [
    "### 4. function for train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "988289e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_grapheme_bpe(train_texts, lang, vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Train GraphemeBPE tokenizer from your dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your dataset with structure {'idx': int, 'src': str, 'tgt': str}\n",
    "        vocab_size: Desired vocabulary size\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training on {len(train_texts)} {lang} texts\")\n",
    "    \n",
    "    # Show sample texts and their graphemes\n",
    "    print(f\"\\nSample training text: {train_texts[0]}\")\n",
    "    sample_graphemes = list(grapheme.graphemes(train_texts[0]))\n",
    "    print(f\"Graphemes: {sample_graphemes}\")\n",
    "    print(f\"Number of graphemes: {len(sample_graphemes)}\")\n",
    "    \n",
    "    # Initialize and train tokenizer\n",
    "    tokenizer = GraphemeBPETokenizer()\n",
    "    print(f\"\\nTraining tokenizer with vocab_size={vocab_size}...\")\n",
    "    tokenizer.train(train_texts, vocab_size=vocab_size)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"{lang}_grapheme_bpe.json\"\n",
    "    tokenizer.save(model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    # Show vocabulary statistics\n",
    "    print(f\"\\nVocabulary Statistics:\")\n",
    "    print(f\"Total vocabulary size: {len(tokenizer.vocab)}\")\n",
    "    print(f\"Number of merges performed: {len(tokenizer.merges)}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88775ebd",
   "metadata": {},
   "source": [
    "### 5. function for train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28839aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hf_bpe_tokenizer(train_texts, lang, vocab_size=1000):\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator(train_texts, trainer)\n",
    "\n",
    "    tokenizer.save(f\"{lang}_hf_bpe.json\")\n",
    "    print(\"\\nHuggingFace BPE tokenizer saved.\")\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb845ea1",
   "metadata": {},
   "source": [
    "### 6. Train Graphene-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e99dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 8000 tamil texts\n",
      "\n",
      "Sample training text: ‡Æö‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç 14 ‡ÆÆ‡Ææ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ, ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‡Æï‡Æ©‡Øç‡Æ±‡Øà ‡Æà‡Æ©‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.\n",
      "Graphemes: ['‡Æö‡ØÅ', '‡ÆÆ‡Ææ', '‡Æ∞‡Øç', ' ', '1', '4', ' ', '‡ÆÆ‡Ææ', '‡Æ§', '‡Æô‡Øç', '‡Æï', '‡Æ≥‡Øç', ' ', '‡Æï', '‡Æ¥‡Æø', '‡Æ§‡Øç', '‡Æ§‡ØÅ', ',', ' ', '‡Æá', '‡Æ∞', '‡Æ£‡Øç', '‡Æü‡Ææ', '‡ÆÆ‡Øç', ' ', '‡Æï', '‡Æ©‡Øç', '‡Æ±‡Øà', ' ', '‡Æà', '‡Æ©‡ØÅ', '‡Æï‡Æø', '‡Æ±', '‡Æ§‡ØÅ', '.']\n",
      "Number of graphemes: 35\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 188\n",
      "Sample graphemes: ['‡Æö', '‡ØÅ', '‡ÆÆ', '‡Ææ', '‡Æ∞', '‡Øç', '<', '/', 'w', '>', ' ', '1', '4', '‡Æ§', '‡Æô', '‡Æï', '‡Æ≥', '‡Æ¥', '‡Æø', ',']\n",
      "Merge 0: ('‡ÆÆ‡Øç', '</w>') -> ‡ÆÆ‡Øç</w> (freq: 6861)\n",
      "Merge 100: ('0', '</w>') -> 0</w> (freq: 368)\n",
      "Merge 200: ('‡Æö‡ØÜ‡ÆØ‡Øç', '‡ÆØ') -> ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ (freq: 209)\n",
      "Merge 300: ('‡Æµ‡Æø', '‡Æ∞‡ØÅ') -> ‡Æµ‡Æø‡Æ∞‡ØÅ (freq: 140)\n",
      "Merge 400: ('‡ÆØ‡ØÜ', '‡Æï‡Øã‡Æµ‡Ææ') -> ‡ÆØ‡ØÜ‡Æï‡Øã‡Æµ‡Ææ (freq: 111)\n",
      "Merge 500: ('‡Æï‡Øç', '‡Æï‡ØÇ') -> ‡Æï‡Øç‡Æï‡ØÇ (freq: 89)\n",
      "Merge 600: ('‡Æ™‡Øç‡Æ™‡Øà', '</w>') -> ‡Æ™‡Øç‡Æ™‡Øà</w> (freq: 75)\n",
      "Merge 700: ('‡Æ≥‡Ææ', '‡Æ≤‡Øç</w>') -> ‡Æ≥‡Ææ‡Æ≤‡Øç</w> (freq: 62)\n",
      "Merge 800: ('‡Æµ', '‡Æô‡Øç‡Æï‡Æø') -> ‡Æµ‡Æô‡Øç‡Æï‡Æø (freq: 55)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: tamil_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 812\n",
      "\n",
      "üîπ GPE Compression Ratio: 2.924777121008174\n"
     ]
    }
   ],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_ta = train_grapheme_bpe(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_ta = calculate_compression_ratio(gpe_tokenizer_ta, eval_texts_ta)\n",
    "print(\"\\nüîπ GPE Compression Ratio:\", gpe_stats_ta['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3d93e",
   "metadata": {},
   "source": [
    "### 7. Train Byte-pair encoding tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a5ae228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "üîπ HF BPE Compression Ratio: 2.9452891717743204\n"
     ]
    }
   ],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_ta = train_hf_bpe_tokenizer(train_texts_ta, \"tamil\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_ta = calculate_compression_ratio(hf_tokenizer_ta, eval_texts_ta)\n",
    "print(\"üîπ HF BPE Compression Ratio:\", hf_stats_ta['compression_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250a03d",
   "metadata": {},
   "source": [
    "### 8. Overview of comparission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4849e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.92\n",
      "HuggingFace BPE:    2.95\n"
     ]
    }
   ],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\nüìä Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_ta['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_ta['compression_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e2f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala Flores+ dataset\n",
    "sin_ds = load_dataset(\"openlanguagedata/flores_plus\", \"sin_Sinh\")\n",
    "train_texts_si = [item['text'] for item in sin_ds['dev']]\n",
    "eval_texts_si = [item['text'] for item in sin_ds['devtest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06becad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 997 sinhala texts\n",
      "\n",
      "Sample training text: ‡∑É‡∂≥‡∑î‡∂Ø‡∑è ‡∂Ø‡∑í‡∂±, ‡∑É‡∑ä‡∂ß‡∑ê‡∂±‡∑ä‡∑Ü‡∂ª‡∑ä‡∂©‡∑ä ‡∑É‡∂ª‡∑É‡∑Ä‡∑í ‡∑Ä‡∑õ‡∂Ø‡∑ä‚Äç‡∂∫ ‡∂¥‡∑è‡∑É‡∂Ω‡∑ö ‡∑Ä‡∑í‡∂Ø‡∑ä‚Äç‡∂∫‡∑è‡∂•‡∂∫‡∑ù, ‡∑É‡∑õ‡∂Ω‡∂∫‡∂ö ‡∂Ü‡∂ö‡∑è‡∂ª‡∂∫ ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂ë‡∂∫ ‡∑Ä‡∂ª‡∑ä‡∂ú ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂±‡∑Ä ‡∂Ø‡∑ù‡∑Ç ‡∂±‡∑í‡∂ª‡∑ä‡∂´‡∂∫ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ö ‡∂∏‡∑ô‡∑Ä‡∂Ω‡∂∏‡∂ö‡∑ä: ‡∂ë‡∂±‡∂∏‡∑ä, ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂á‡∂∏‡∂ª‡∑í‡∂ö‡∑è‡∂±‡∑î ‡∂©‡∑ú‡∂Ω‡∂ª‡∑ä ‡∑Å‡∂≠‡∂∫‡∂ö ‡∂¥‡∂∏‡∂´ ‡∂∏‡∑î‡∂Ø‡∂Ω‡∂ö‡∑í‡∂±‡∑ä ‡∂±‡∑í‡∑Ç‡∑ä‡∂¥‡∑è‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∑É‡∑è‡∂∏‡∑è‡∂±‡∑ä‚Äç‡∂∫ ‡∂â‡∂±‡∑ä‡∂ö‡∑ä‡∂¢‡∑ô‡∂ß‡∑ä ‡∂∏‡∑î‡∂Ø‡∑ä‚Äç‡∂ª‡∂´ ‡∂∫‡∂±‡∑ä‡∂≠‡∑ä‚Äç‡∂ª‡∂∫‡∂ö‡∑í‡∂±‡∑ä ‡∂∏‡∑î‡∂Ø‡∑ä‚Äç‡∂ª‡∂´‡∂∫ ‡∂ö‡∑Ö ‡∑Ñ‡∑ê‡∂ö‡∑í ‡∂â‡∂≠‡∑è ‡∂ö‡∑î‡∂©‡∑è ‡∂†‡∑í‡∂¥‡∂∫‡∂ö‡∑ä ‡∑É‡∑ú‡∂∫‡∑è‡∂ú‡∂≠‡∑ä ‡∂∂‡∑Ä‡∂ß ‡∂±‡∑í‡∑Ä‡∑ö‡∂Ø‡∂±‡∂∫ ‡∂ö‡∑Ö‡∑Ñ.\n",
      "Graphemes: ['‡∑É', '‡∂≥‡∑î', '‡∂Ø‡∑è', ' ', '‡∂Ø‡∑í', '‡∂±', ',', ' ', '‡∑É‡∑ä', '‡∂ß‡∑ê', '‡∂±‡∑ä', '‡∑Ü', '‡∂ª‡∑ä', '‡∂©‡∑ä', ' ', '‡∑É', '‡∂ª', '‡∑É', '‡∑Ä‡∑í', ' ', '‡∑Ä‡∑õ', '‡∂Ø‡∑ä\\u200d', '‡∂∫', ' ', '‡∂¥‡∑è', '‡∑É', '‡∂Ω‡∑ö', ' ', '‡∑Ä‡∑í', '‡∂Ø‡∑ä\\u200d', '‡∂∫‡∑è', '‡∂•', '‡∂∫‡∑ù', ',', ' ', '‡∑É‡∑õ', '‡∂Ω', '‡∂∫', '‡∂ö', ' ', '‡∂Ü', '‡∂ö‡∑è', '‡∂ª', '‡∂∫', ' ', '‡∂Ö', '‡∂±‡∑î', '‡∑Ä', ' ', '‡∂ë', '‡∂∫', ' ', '‡∑Ä', '‡∂ª‡∑ä', '‡∂ú', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∂±', '‡∑Ä', ' ', '‡∂Ø‡∑ù', '‡∑Ç', ' ', '‡∂±‡∑í', '‡∂ª‡∑ä', '‡∂´', '‡∂∫', ' ', '‡∂ö‡∑í', '‡∂ª‡∑ì', '‡∂∏‡∑ö', ' ', '‡∂∏‡∑ô', '‡∑Ä', '‡∂Ω', '‡∂∏', '‡∂ö‡∑ä', ':', ' ', '‡∂ë', '‡∂±', '‡∂∏‡∑ä', ',', ' ', '‡∂ë', '‡∂ö', '‡∂ö‡∑ä', ' ', '‡∂á', '‡∂∏', '‡∂ª‡∑í', '‡∂ö‡∑è', '‡∂±‡∑î', ' ', '‡∂©‡∑ú', '‡∂Ω', '‡∂ª‡∑ä', ' ', '‡∑Å', '‡∂≠', '‡∂∫', '‡∂ö', ' ', '‡∂¥', '‡∂∏', '‡∂´', ' ', '‡∂∏‡∑î', '‡∂Ø', '‡∂Ω', '‡∂ö‡∑í', '‡∂±‡∑ä', ' ', '‡∂±‡∑í', '‡∑Ç‡∑ä', '‡∂¥‡∑è', '‡∂Ø', '‡∂±', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∑É‡∑è', '‡∂∏‡∑è', '‡∂±‡∑ä\\u200d', '‡∂∫', ' ', '‡∂â', '‡∂±‡∑ä', '‡∂ö‡∑ä', '‡∂¢‡∑ô', '‡∂ß‡∑ä', ' ', '‡∂∏‡∑î', '‡∂Ø‡∑ä\\u200d', '‡∂ª', '‡∂´', ' ', '‡∂∫', '‡∂±‡∑ä', '‡∂≠‡∑ä\\u200d', '‡∂ª', '‡∂∫', '‡∂ö‡∑í', '‡∂±‡∑ä', ' ', '‡∂∏‡∑î', '‡∂Ø‡∑ä\\u200d', '‡∂ª', '‡∂´', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', ' ', '‡∑Ñ‡∑ê', '‡∂ö‡∑í', ' ', '‡∂â', '‡∂≠‡∑è', ' ', '‡∂ö‡∑î', '‡∂©‡∑è', ' ', '‡∂†‡∑í', '‡∂¥', '‡∂∫', '‡∂ö‡∑ä', ' ', '‡∑É‡∑ú', '‡∂∫‡∑è', '‡∂ú', '‡∂≠‡∑ä', ' ', '‡∂∂', '‡∑Ä', '‡∂ß', ' ', '‡∂±‡∑í', '‡∑Ä‡∑ö', '‡∂Ø', '‡∂±', '‡∂∫', ' ', '‡∂ö', '‡∑Ö', '‡∑Ñ', '.']\n",
      "Number of graphemes: 197\n",
      "\n",
      "Training tokenizer with vocab_size=1000...\n",
      "Starting grapheme-based BPE training...\n",
      "Initial vocabulary size: 158\n",
      "Sample graphemes: ['‡∑É', '‡∂≥', '‡∑î', '‡∂Ø', '‡∑è', '<', '/', 'w', '>', ' ', '‡∑í', '‡∂±', ',', '‡∑ä', '‡∂ß', '‡∑ê', '‡∑Ü', '‡∂ª', '‡∂©', '‡∑Ä']\n",
      "Merge 0: ('‡∂∫', '</w>') -> ‡∂∫</w> (freq: 1469)\n",
      "Merge 100: ('‡∂¥', '‡∑Ä') -> ‡∂¥‡∑Ä (freq: 82)\n",
      "Merge 200: ('‡∂ú', '‡∂≠‡∑ä</w>') -> ‡∂ú‡∂≠‡∑ä</w> (freq: 47)\n",
      "Merge 300: ('‡∂ë', '‡∂∏</w>') -> ‡∂ë‡∂∏</w> (freq: 34)\n",
      "Merge 400: ('‡∂Ö', '‡∂©‡∑î</w>') -> ‡∂Ö‡∂©‡∑î</w> (freq: 24)\n",
      "Merge 500: ('‡∑Å', '‡∂ö‡∑ä') -> ‡∑Å‡∂ö‡∑ä (freq: 20)\n",
      "Merge 600: ('‡∂ö‡∑è', '‡∂ª‡∑ä') -> ‡∂ö‡∑è‡∂ª‡∑ä (freq: 17)\n",
      "Merge 700: ('‡∂ö‡∂ª', '‡∂±‡∑ä‡∂±‡∑ö</w>') -> ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∑ö</w> (freq: 14)\n",
      "Merge 800: ('‡∂Ö', '‡∂ª‡∑ä') -> ‡∂Ö‡∂ª‡∑ä (freq: 12)\n",
      "Final vocabulary size: 1000\n",
      "Training completed!\n",
      "\n",
      "Model saved to: sinhala_grapheme_bpe.json\n",
      "\n",
      "Vocabulary Statistics:\n",
      "Total vocabulary size: 1000\n",
      "Number of merges performed: 842\n",
      "\n",
      "üîπ GPE Compression Ratio: 2.4359331735147087\n"
     ]
    }
   ],
   "source": [
    "# Train GPE tokenizer\n",
    "gpe_tokenizer_si = train_grapheme_bpe(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate GPE tokenizer\n",
    "gpe_stats_si = calculate_compression_ratio(gpe_tokenizer_si, eval_texts_si)\n",
    "print(\"\\nüîπ GPE Compression Ratio:\", gpe_stats_si['compression_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a565b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace BPE tokenizer saved.\n",
      "üîπ HF BPE Compression Ratio: 2.6817242578612492\n"
     ]
    }
   ],
   "source": [
    "# Train HF BPE tokenizer\n",
    "hf_tokenizer_si = train_hf_bpe_tokenizer(train_texts_si, \"sinhala\", vocab_size=1000)\n",
    "\n",
    "# Evaluate HF tokenizer\n",
    "hf_stats_si = calculate_compression_ratio(hf_tokenizer_si, eval_texts_si)\n",
    "print(\"üîπ HF BPE Compression Ratio:\", hf_stats_si['compression_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6199bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Compression Ratio Comparison\n",
      "GPE Tokenizer:      2.44\n",
      "HuggingFace BPE:    2.68\n"
     ]
    }
   ],
   "source": [
    "# ---- Final Comparison ----\n",
    "print(\"\\nüìä Compression Ratio Comparison\")\n",
    "print(f\"GPE Tokenizer:      {gpe_stats_si['compression_ratio']:.2f}\")\n",
    "print(f\"HuggingFace BPE:    {hf_stats_si['compression_ratio']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
